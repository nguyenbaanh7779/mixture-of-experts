{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_path = \"\"\n",
    "for path in os.getcwd().split(\"\\\\\")[:-1]:\n",
    "    root_path += f\"{path}/\"\n",
    "sys.path.insert(1, root_path)\n",
    "sys.path.insert(1, os.path.join(root_path, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\OneDrive - Hanoi University of Science and Technology\\DANC\\source_code\\py_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(dictionary, path, limit_line=None):\n",
    "    nb_tokens_in_dictionary = len(dictionary)\n",
    "    # load document to tokenize\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        document = f.read()\n",
    "\n",
    "    # Count nb of tokens in text and update the dictionary\n",
    "    for i, line in enumerate(tqdm(document, desc=\"Creating dictionary\", unit=\" lines\")):\n",
    "        if i == limit_line:\n",
    "            break\n",
    "        tokens = line.split() + [\"<eos>\"]\n",
    "        for token in tokens:\n",
    "            if token not in dictionary:\n",
    "                dictionary[token] = nb_tokens_in_dictionary\n",
    "                nb_tokens_in_dictionary += 1\n",
    "\n",
    "    # Assign to each token its identifier\n",
    "    ids = []\n",
    "    for i, line in enumerate(tqdm(document, desc=\"Encoding token\", unit=\" lines\")):\n",
    "        if i == limit_line:\n",
    "            break\n",
    "        i += 1\n",
    "        tokens = line.split() + [\"<eos>\"]\n",
    "        for token in tokens:\n",
    "            ids.append(dictionary[token])\n",
    "    ids = torch.LongTensor(ids)\n",
    "    return ids\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, path=None):\n",
    "        self._dictionary = {}\n",
    "        print(\"Processing train ...\")\n",
    "        self.train = _tokenize(\n",
    "            dictionary=self._dictionary, path=os.path.join(path, \"train.txt\")\n",
    "        )\n",
    "        print(\"Processing valid ...\")\n",
    "        self.validation = _tokenize(\n",
    "            dictionary=self._dictionary, path=os.path.join(path, \"validation.txt\")\n",
    "        )\n",
    "        print(\"Processing test ...\")\n",
    "        self.test = _tokenize(\n",
    "            dictionary=self._dictionary, path=os.path.join(path, \"test.txt\")\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: torch.Tensor, batch_size):\n",
    "    # Tính số batch trên data\n",
    "    num_batches = data.size(0) // batch_size\n",
    "    # Lấy đủ số lượng batch có thể lấy trên dữ liệu và cắt bỏ những dữ liệu cuối\n",
    "    data = data.narrow(0, 0, num_batches * batch_size)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_nlp(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module with an encoder, a recurrent module, and a decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nonlinearity, ntoken, ninp, nhid, nlayer, dropout=0.5, tie_weights=False):\n",
    "        super(RNN_nlp, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.RNN(ninp, nhid, nlayer, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayer\n",
    "\n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, i, stride, evaluation=False):\n",
    "    seq_len = min(stride, len(data) - 1 - i)\n",
    "    inputs = data[i : i + seq_len]\n",
    "    targets = data[i + 1 : i + 1 + seq_len].view(-1)\n",
    "\n",
    "    if evaluation:\n",
    "        # Đảm bảo không cần theo dõi gradient\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.clone()\n",
    "            targets = targets.clone()\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, criterion, optimize, num_epochs=5, **train_params):\n",
    "    batch_size = train_params.get(\"batch_size\", train_data.size(0))\n",
    "    n_token = train_params.get(\"n_token\", model.decoder.out_features)\n",
    "    stride = train_params.get(\"stride\", 32)\n",
    "    log_interval = train_params.get(\"log_interval\", 500)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(bsz=batch_size)\n",
    "\n",
    "        for batch, i in enumerate(tqdm(range(0, train_data.size(0), stride), desc=f\"Epoch {epoch}|\", unit=\" batchs\")):\n",
    "            inputs, target = get_batch(data=train_data, i=i, stride=stride)\n",
    "            inputs, target = inputs.to(device=device), target.to(device=device)\n",
    "            print(inputs.shape, hidden.shape)\n",
    "\n",
    "            model.zero_grad()\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            print(outputs.shape, target.view(-1).shape)\n",
    "            loss = criterion(outputs.view(-1, n_token), target.view(-1))\n",
    "\n",
    "            if batch % log_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{num_epochs} | Batch {batch}/{int(len(train_data) / stride)} | \"\n",
    "                    f\"ms/batch: {elapsed:.2f} | loss: {loss.item():.4f} | ppl: {torch.exp(loss).item():.4f}\"\n",
    "                )\n",
    "                start_time = time.time()  # Reset timer after logging\n",
    "            return 0\n",
    "            loss.backward()\n",
    "            optimize.step()\n",
    "            optimize.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exited\n",
      "Loading corpus ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20024\\3173859826.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  corpus = torch.load(os.path.join(root_path, \"data/wikitext-103/corpus.pt\"))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(root_path, \"data/wikitext-103/\")):\n",
    "    print(\"Data exited\")\n",
    "else:\n",
    "    print(\"Loading data set ...\")\n",
    "    ds = datasets.load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "    for type_ds in [\"train\", \"validation\", \"test\"]:\n",
    "        lines = ds[type_ds][\"text\"]\n",
    "        if not os.path.exists(os.path.join(root_path, \"data/wikitext-103\")):\n",
    "            os.makedirs(os.path.join(root_path, \"data/wikitext-103\"))\n",
    "        with open(os.path.join(root_path, f\"data/wikitext-103/{type_ds}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm(lines, desc=f\"Saving {type_ds}.txt\", unit=\" lines\"):\n",
    "                f.write(line)\n",
    "        f.close()\n",
    "\n",
    "if os.path.exists(os.path.join(root_path, \"data/wikitext-103/corpus.pt\")):\n",
    "    print(\"Loading corpus ...\")\n",
    "    corpus = torch.load(os.path.join(root_path, \"data/wikitext-103/corpus.pt\"))\n",
    "else:\n",
    "    print(\"Creating corpus ...\")\n",
    "    corpus = Corpus(path=os.path.join(root_path, \"data/wikitext-103\"))\n",
    "    torch.save(corpus, os.path.join(root_path, \"data/wikitext-103/corpus.pt\"))\n",
    "\n",
    "train_data = batchify(data=corpus.train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0|:   0%|          | 0/7601 [00:00<?, ? batchs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 128]) torch.Size([3, 128, 32])\n",
      "torch.Size([1000, 128, 5007]) torch.Size([128000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0|:   0%|          | 0/7601 [00:33<?, ? batchs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Batch 0/7600 | ms/batch: 32.87 | loss: 8.5296 | ppl: 5062.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(root_path, \"config/model_params.yaml\"),  \"r\") as f:\n",
    "    model_params = yaml.safe_load(f)\n",
    "f.close()\n",
    "model_params['ntoken'] = corpus.vocab_size\n",
    "batch_size = 128\n",
    "model = RNN_nlp(**model_params)\n",
    "\n",
    "with open(os.path.join(root_path, \"config/train_params.yaml\"), \"r\") as f:\n",
    "    train_params = yaml.safe_load(f)\n",
    "\n",
    "train_params.update(\n",
    "    {\n",
    "        \"criterion\": nn.CrossEntropyLoss(),\n",
    "        \"optimize\": torch.optim.SGD(model.parameters(), lr=float(train_params[\"lr\"])),\n",
    "        \"n_token\": corpus.vocab_size,\n",
    "        \"batch_size\": batch_size,\n",
    "    } \n",
    ")\n",
    "train(model= model, train_data=train_data, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_epochs': 5,\n",
       " 'stride': 1000,\n",
       " 'lr': '1e-3',\n",
       " 'batch_size': 128,\n",
       " 'criterion': CrossEntropyLoss(),\n",
       " 'optimize': SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     differentiable: False\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'n_token': 5007}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
